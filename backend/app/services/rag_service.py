# app/services/rag_service.py

import os
from typing import List, Dict
from langchain_google_genai import ChatGoogleGenerativeAI
from app.vector.store import build_store
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import RetrievalQA
from langchain.docstore.document import Document
from tempfile import NamedTemporaryFile
import unicodedata
import logging
from .resume_parser import parse_entire_resume  # Import the function
import re

# ðŸ”¹ NEW: Sentence tokenizer
from nltk.tokenize import sent_tokenize
import nltk
nltk.download('punkt')

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")


def clean_text_for_embedding(text: str) -> str:
    normalized_text = unicodedata.normalize('NFKC', text)
    cleaned_chars = []
    for char in normalized_text:
        try:
            char.encode('utf-8')
            cleaned_chars.append(char)
        except UnicodeEncodeError:
            cleaned_chars.append(' ')
    return "".join(cleaned_chars)


def load_internal_docs(data_folder="app/data") -> List[Document]:
    all_docs = []
    for filename in os.listdir(data_folder):
        if filename.endswith(".pdf"):
            loader = PyPDFLoader(os.path.join(data_folder, filename))
            pages = loader.load()
            for page in pages:
                page.page_content = clean_text_for_embedding(page.page_content)
            all_docs.extend(pages)
    return all_docs


def load_uploaded_resume(uploaded_file_bytes: bytes) -> List[Document]:
    with NamedTemporaryFile(delete=False, suffix=".pdf") as temp_pdf:
        temp_pdf.write(uploaded_file_bytes)
        temp_pdf_path = temp_pdf.name

    loader = PyPDFLoader(temp_pdf_path)
    docs = loader.load()

    os.remove(temp_pdf_path)

    for doc in docs:
        doc.page_content = clean_text_for_embedding(doc.page_content)

    return docs
#custom splitter for resume
def custom_resume_splitter(text: str) -> Dict[str, str]:
    """
    Splits resume text into chunks based on given keywords (section headers).
    
    Args:
        text (str): The entire resume text.
        keywords (List[str]): Section headers to split on (case-insensitive).
    
    Returns:
        Dict[str, str]: A dictionary with section names as keys and their content as values.
    """
    keywords = ["SUMMARY", "EDUCATION", "PROJECTS", "RELEVANT SKILLS", "SKILLS", "CERTIFICATIONS", "ADDITIONAL INFORMATION"]
    
    # Normalize text
    text = text.replace('\n', '\n\n')  # Separate lines visually for regex reliability

    # Create regex pattern for section titles
    pattern = r'(?i)^(' + '|'.join(re.escape(k) for k in keywords) + r')\s*$'
    
    # Find all matches
    matches = list(re.finditer(pattern, text, flags=re.MULTILINE))

    #Extracting name
    name = ''
    for i in range(50):
        if text[i] == "\n":
            break
        name += text[i]
    
    # Extract sections
    chunks = {}
    chunks['Name'] = name
    
    for i, match in enumerate(matches):
        start = match.end()
        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
        section = match.group(1).strip().title()
        chunks[section] = text[start:end].strip()
    
    return chunks

# ðŸ”¹ NEW: Semantic chunking function (replaces SemanticChunker)
def semantic_chunk(documents: List[Document], chunk_size: int = 3) -> List[Document]:
    chunks = []
    for doc in documents:
        sentences = sent_tokenize(doc.page_content)
        for i in range(0, len(sentences), chunk_size):
            chunk_text = " ".join(sentences[i:i + chunk_size])
            chunks.append(Document(page_content=chunk_text, metadata=doc.metadata))
    return chunks


def build_vector_store(documents: List[Document], desc: str = "documents"):
    logger.info(f"Chunking documents for {desc}")
    chunks = semantic_chunk(documents, chunk_size=3)
    logger.info(f"Creating vector store for {desc} with {len(chunks)} chunks.")
    return build_store(chunks, collection_name=f"{desc}_collection")

def build_vector_store_resume(documents: List[Document], desc: str = "documents"):
    logger.info(f"Chunking documents for {desc}")
    # NOTE: custom_resume_splitter returns dict of sections; convert to Document objects
    # If documents is list of Document (pages), merge their content first.
    merged_text = "\n".join(doc.page_content for doc in documents)
    sections = custom_resume_splitter(merged_text)
    section_docs = [Document(page_content=v, metadata={"section": k}) for k, v in sections.items()]
    logger.info(f"Creating vector store for {desc} with {len(section_docs)} chunks.")
    return build_store(section_docs, collection_name=f"{desc}_resume")

def generate_interview_with_resume(uploaded_resume_bytes: bytes, filename: str, query: str) -> str:
    try:
        internal_docs = load_internal_docs()
        resume_docs = load_uploaded_resume(uploaded_resume_bytes)

        # 1. Parse the entire resume
        resume_data = parse_entire_resume(uploaded_resume_bytes, filename)

        if not resume_data.get("success"):
            return f"Error parsing resume: {resume_data.get('error', 'Unknown error')}"

        skills = resume_data["extracted_data"].get("skills", [])
        roles = resume_data["extracted_data"].get("roles", [])
        full_resume_text = resume_data.get("full_text", "")

        # 2. Build separate vector stores
        resume_vector_store = build_vector_store_resume(resume_docs, "resume")
        internal_vector_store = build_vector_store(internal_docs, "internal docs")

        # 3. Initial query of resume
        resume_retriever = resume_vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 3})
        resume_context = resume_retriever.get_relevant_documents(query)
        resume_context_text = "\n".join([doc.page_content for doc in resume_context])

        # 4. Generate contextual queries based on resume
        contextual_queries = []
        if skills:
            contextual_queries.append(f"{query} related to {', '.join(skills)}")
        if roles:
            contextual_queries.append(f"{query} for a {', '.join(roles)}")
        contextual_queries.append(f"{query} given the following resume context: {full_resume_text}")

        # 5. Query internal docs with contextual queries
        internal_context_docs = []
        for q in contextual_queries:
            internal_context_docs.extend(internal_vector_store.similarity_search(q, k=2))
        internal_context_text = "\n".join([doc.page_content for doc in internal_context_docs])

        # 6. Combine and prioritize context
        combined_context = f"Resume Context:\n{resume_context_text}\n\nInternal Document Context:\n{internal_context_text}"

        # 7. Use ChatGoogleGenerativeAI for LLM
        llm = ChatGoogleGenerativeAI(model="gemini-pro", google_api_key=GOOGLE_API_KEY)

        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=internal_vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 5}),
        )

        result = qa_chain({"query": f"{query}\nContext:{combined_context}"})
        return result["result"]

    except Exception as e:
        logger.error(f"Error in generate_interview_with_resume: {e}", exc_info=True)
        return f"An error occurred: {e}"
